{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset financial_phrasebank/sentences_50agree to C:/Users/Gusso/.cache/huggingface/datasets/financial_phrasebank/sentences_50agree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset financial_phrasebank downloaded and prepared to C:/Users/Gusso/.cache/huggingface/datasets/financial_phrasebank/sentences_50agree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 83.32it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"financial_phrasebank\", 'sentences_50agree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negatives 604\n",
      "number of neutral 2879\n",
      "number of positives 1363\n"
     ]
    }
   ],
   "source": [
    "value_count_0=dataset['train']['label'].count(0)\n",
    "value_count_1=dataset['train']['label'].count(1)\n",
    "value_count_2=dataset['train']['label'].count(2)\n",
    "\n",
    "\n",
    "print('number of negatives',value_count_0)\n",
    "print('number of neutral',value_count_1)\n",
    "print('number of positives',value_count_2)\n",
    "#count_1=value_count['1'] # if it is str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Gusso\\.cache\\huggingface\\datasets\\financial_phrasebank\\sentences_50agree\\1.0.0\\550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141\\cache-4069f8bdc2072a81.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Gusso\\.cache\\huggingface\\datasets\\financial_phrasebank\\sentences_50agree\\1.0.0\\550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141\\cache-255302c37e6b94c9.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Gusso\\.cache\\huggingface\\datasets\\financial_phrasebank\\sentences_50agree\\1.0.0\\550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141\\cache-913aca77f5bc0114.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'label'],\n",
      "    num_rows: 604\n",
      "})\n",
      "Dataset({\n",
      "    features: ['sentence', 'label'],\n",
      "    num_rows: 1208\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "neutrals = dataset.filter(lambda example: example[\"label\"] == 1)\n",
    "positives = dataset.filter(lambda example: example[\"label\"] == 2)\n",
    "negatives = dataset.filter(lambda example: example[\"label\"] == 0)\n",
    "\n",
    "short_positives = positives['train'].select(range(len(negatives['train'])))\n",
    "\n",
    "print(short_positives)\n",
    "\n",
    "downsized_dataset = concatenate_datasets([short_positives, negatives['train']])\n",
    "\n",
    "print(downsized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Gusso\\.cache\\huggingface\\datasets\\financial_phrasebank\\sentences_50agree\\1.0.0\\550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141\\cache-005d6f20e6ad7c92.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label'],\n",
      "        num_rows: 1087\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label'],\n",
      "        num_rows: 121\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def replace_values(example):\n",
    "    # Replace values in the label column\n",
    "    if \"label\" in example:\n",
    "        example[\"label\"] = 1 if example[\"label\"] == 2 else example[\"label\"]\n",
    "    return example\n",
    "\n",
    "# Use the map function to apply the custom mapping\n",
    "downsized_dataset = downsized_dataset.map(replace_values)\n",
    "downsized_dataset = downsized_dataset.train_test_split(test_size=0.1, shuffle=True)\n",
    "print(downsized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = downsized_dataset['train'].map(tokenize_function, batched=True)\n",
    "tokenized_datasets_eval = downsized_dataset['test'].map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/408 [03:31<?, ?it/s]\n",
      " 20%|██        | 83/408 [01:03<04:08,  1.31it/s]\n",
      "                                                 \n",
      " 33%|███▎      | 136/408 [00:40<01:16,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2037314474582672, 'eval_accuracy': 0.9421487603305785, 'eval_runtime': 1.8825, 'eval_samples_per_second': 64.276, 'eval_steps_per_second': 8.499, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|██████▋   | 272/408 [01:21<00:37,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22784696519374847, 'eval_accuracy': 0.9504132231404959, 'eval_runtime': 1.855, 'eval_samples_per_second': 65.229, 'eval_steps_per_second': 8.625, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 408/408 [02:02<00:00,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22509229183197021, 'eval_accuracy': 0.9504132231404959, 'eval_runtime': 1.8455, 'eval_samples_per_second': 65.564, 'eval_steps_per_second': 8.67, 'epoch': 3.0}\n",
      "{'train_runtime': 122.9653, 'train_samples_per_second': 26.52, 'train_steps_per_second': 3.318, 'train_loss': 0.13773220660639743, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=408, training_loss=0.13773220660639743, metrics={'train_runtime': 122.9653, 'train_samples_per_second': 26.52, 'train_steps_per_second': 3.318, 'train_loss': 0.13773220660639743, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Gusso/.cache/huggingface/datasets/zeroshot___csv/zeroshot--twitter-financial-news-sentiment-ccca0f3c622c5b67/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 2/2 [00:00<00:00, 1001.03it/s]\n",
      "Loading cached processed dataset at C:\\Users\\Gusso\\.cache\\huggingface\\datasets\\zeroshot___csv\\zeroshot--twitter-financial-news-sentiment-ccca0f3c622c5b67\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-b2d98c45395bc49c.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Gusso\\.cache\\huggingface\\datasets\\zeroshot___csv\\zeroshot--twitter-financial-news-sentiment-ccca0f3c622c5b67\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-98a6e9e55fa382e5.arrow\n",
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label'],\n",
      "        num_rows: 4846\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "validation_dataset = load_dataset('zeroshot/twitter-financial-news-sentiment')\n",
    "\n",
    "def filter_function(example):\n",
    "    return example[\"label\"] != 2\n",
    "\n",
    "# Use the filter method to remove records\n",
    "filtered_dataset = validation_dataset.filter(filter_function)\n",
    "\n",
    "def tokenize_function2(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets_validation = filtered_dataset.map(tokenize_function2, batched=True)\n",
    "\n",
    "# Print the modified dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:12<00:00,  8.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9002207517623901,\n",
       " 'eval_accuracy': 0.791970802919708,\n",
       " 'eval_runtime': 12.8695,\n",
       " 'eval_samples_per_second': 63.872,\n",
       " 'eval_steps_per_second': 8.003,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_datasets_validation['validation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging_torch_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
